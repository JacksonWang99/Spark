{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`written by Jackson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Python list to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化环境\n",
    "#spark 的学习直接查看官方文档\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\") \n",
    "#Rdd 的入口\n",
    "sc = SparkContext(conf = conf) \n",
    "#Datafram 的入口\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How  to creat Datafram, there is the wsceond method using RDD\n",
    "trainingData = [[\"Chinese Beijing Chinese\", \"c\"],\\\n",
    "                [\"Chinese Chinese Nanjing\", \"c\"],\\\n",
    "                [\"Chinese Macao\", \"c\"],\\\n",
    "                [\"Australia Sydney Chinese\",\"o\"],\\\n",
    "               ]\n",
    "testData = [\"Chinese Chinese Chinese Australia Sydney\"]\n",
    "type(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chinese Beijing Chinese', 'c'],\n",
       " ['Chinese Chinese Nanjing', 'c'],\n",
       " ['Chinese Macao', 'c'],\n",
       " ['Australia Sydney Chinese', 'o']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD = sc.parallelize(trainingData)\n",
    "testRDD = sc.parallelize(testData)\n",
    "trainRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='c', descript='Chinese Beijing Chinese'),\n",
       " Row(category='c', descript='Chinese Chinese Nanjing'),\n",
       " Row(category='c', descript='Chinese Macao'),\n",
       " Row(category='o', descript='Australia Sydney Chinese')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bulid RDD\n",
    "trainRDD = sc.parallelize(trainingData)\n",
    "testRDD = sc.parallelize(testData)\n",
    "# 把RDD 的每一行转化成行Row()的类型，为DataFram 做准备\n",
    "trainRDD = trainRDD.map(lambda e: Row(descript=e[0], category=e[1]))\n",
    "testRDD = testRDD.map(lambda e: Row(descript=e))\n",
    "trainRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(descript='Chinese Chinese Chinese Australia Sydney')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|category|descript                |\n",
      "+--------+------------------------+\n",
      "|c       |Chinese Beijing Chinese |\n",
      "|c       |Chinese Chinese Nanjing |\n",
      "|c       |Chinese Macao           |\n",
      "|o       |Australia Sydney Chinese|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# covert to DataFrame，build DtaaFram \n",
    "#createDataFrame function\n",
    "trainDF = spark.createDataFrame(trainRDD)\n",
    "#testDF = testRDD.toDF()\n",
    "testDF = spark.createDataFrame(testRDD)\n",
    "# trainDF.createOrReplaceTempView(\"doc\") for sql \n",
    "\n",
    "#truncate=False 打印表格内容名的全貌\n",
    "trainDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|descript                                |\n",
      "+----------------------------------------+\n",
      "|Chinese Chinese Chinese Australia Sydney|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|       o|    1|\n",
      "|       c|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if we got the above fram, we can do SOL operation anyway\n",
    "#写各种SQL的各种语言\n",
    "# normal operation for df using DSL syntax\n",
    "trainDF.groupby(\"category\").count().show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Tokenizer  把 descript spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+\n",
      "|category|descript                |words                       |\n",
      "+--------+------------------------+----------------------------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|\n",
      "+--------+------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#开始把每句话分成一个个的单词  转化成words\n",
    "#使用Tokenizer包，相当于 split的功能\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "# defined a tokenizer  建立模型\n",
    "#流程化的写法，写法都一样， 传入哪一列，把列名输入进去\n",
    "tokenizer = Tokenizer(inputCol=\"descript\", outputCol=\"words\")\n",
    "\n",
    "# use the tokenizer  开始使用这个模型\n",
    "tokenizedDF = tokenizer.transform(trainDF)\n",
    "tokenizedDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+\n",
      "|category|descript                |words                       |\n",
      "+--------+------------------------+----------------------------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|\n",
      "+--------+------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testTokenizedDF = tokenizer.transform(testDF)\n",
    "# see the result\n",
    "tokenizedDF.select(\"category\", \"descript\", \"words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+------+\n",
      "|category|descript                |words                       |tokens|\n",
      "+--------+------------------------+----------------------------+------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |3     |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |3     |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |2     |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|3     |\n",
      "+--------+------------------------+----------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user defined function\n",
    "countTokens = udf(lambda e: len(e))\n",
    "tokenizedDF = tokenizedDF.select(\"category\", \"descript\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\")))\n",
    "tokenizedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 CountVectorizer 把 words 转换成 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "|category|descript                |words                       |features                 |\n",
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |(6,[0,4],[2.0,1.0])      |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |(6,[0,2],[2.0,1.0])      |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |(6,[0,5],[1.0,1.0])      |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|(6,[0,1,3],[1.0,1.0,1.0])|\n",
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "# 转化成words的时候需要用到define a CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")  # 可以带参数\n",
    "\n",
    "# Estimator fit with DataFrame to get a model, 需要去训练model\n",
    "#fit 的时候就是train的时候\n",
    "cvModel = cv.fit(tokenizedDF)\n",
    "\n",
    "# 把word就转化成了features\n",
    "#.transform 转化成新的DataFram\n",
    "featuredDF = cvModel.transform(tokenizedDF)\n",
    "featuredDF.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            descript|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Chinese Chinese C...|[chinese, chinese...|(6,[0,1,3],[3.0,1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testFeaturedDF =cvModel.transform(testTokenizedDF)\n",
    "testFeaturedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[chinese, australia, sydney,macao, nanjing, beijing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `(6,[0,5],[2.0,1.0])` 是 `sparse Vector` 的形式  \n",
    "* 6 = `vocabulary size`\n",
    "* `[0,5]` 是 index\n",
    "* `[2.0,1.0]` 是 value\n",
    "* 等价于 dense Vector `[2.0, 0.0, 0.0, 0.0, 0.0, 1.0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 StringIndexer 把 label 转换成 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+-------------------------+-----+\n",
      "|category|descript                |words                       |features                 |label|\n",
      "+--------+------------------------+----------------------------+-------------------------+-----+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |(6,[0,4],[2.0,1.0])      |0.0  |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |(6,[0,2],[2.0,1.0])      |0.0  |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |(6,[0,5],[1.0,1.0])      |0.0  |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|(6,[0,1,3],[1.0,1.0,1.0])|1.0  |\n",
      "+--------+------------------------+----------------------------+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导包\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "indexedDF = indexer.fit(featuredDF).transform(featuredDF)\n",
    "indexedDF.show(truncate=False)\n",
    "# c 变成 0\n",
    "# o 变成 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+\n",
      "|features                 |label|\n",
      "+-------------------------+-----+\n",
      "|(6,[0,4],[2.0,1.0])      |0.0  |\n",
      "|(6,[0,2],[2.0,1.0])      |0.0  |\n",
      "|(6,[0,5],[1.0,1.0])      |0.0  |\n",
      "|(6,[0,1,3],[1.0,1.0,1.0])|1.0  |\n",
      "+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDF = indexedDF.select(\"features\", \"label\")\n",
    "finalDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Navie Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意使用 ml 的 NaiveBayes 而不是 mllib 的！！！！！！！\n",
    "#调包使用 Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "# smoothing=1.0, modelType='multinomial' 对应上之前默认的\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label', predictionCol='nb_prediction', smoothing=1.0, modelType='multinomial',)\n",
    "\n",
    "nb_model = nb.fit(finalDF)\n",
    "# head 表示一行\n",
    "#nb_model.transform(testFeaturedDF).show()\n",
    "nb_model.transform(testFeaturedDF).head().nb_prediction  # 0.0 c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|            descript|\n",
      "+--------+--------------------+\n",
      "|       c|Chinese Beijing C...|\n",
      "|       c|Chinese Chinese N...|\n",
      "|       c|       Chinese Macao|\n",
      "|       o|Australia Sydney ...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            descript|\n",
      "+--------------------+\n",
      "|Chinese Chinese C...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|            descript|               words|            features|       rawPrediction|         probability|nb_prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|Chinese Chinese C...|[chinese, chinese...|(6,[0,1,3],[3.0,1...|[-8.2254733485002...|[0.59713120479585...|          0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第一步导包\n",
    "from pyspark.ml import Pipeline\n",
    "#直接把上面的所有的过程放在一起，串成一串\n",
    "nb_pipeline = Pipeline(stages=[tokenizer, cv, indexer, nb])\n",
    "#进行train,然后得到model\n",
    "pipeModel = nb_pipeline.fit(trainDF)\n",
    "#fit 之后我们得到 transformer\n",
    "#使用model,对test数据使用model\n",
    "predictionDF = pipeModel.transform(testDF)\n",
    "predictionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionDF.head().nb_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
