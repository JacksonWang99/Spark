{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deadline + Late Penalty\n",
    "\n",
    "**Note :** It will take you quite some time to complete this project, therefore, we earnestly recommend that you start working as early as possible.\n",
    "\n",
    "\n",
    "* The submission deadline for the Project is **20:59:59 on 9th Aug 2020** (Sydney Time).\n",
    "* **LATE PENALTY: 10% on day-1 and 30% on each subsequent day.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "1. This notebook contains instructions for **COMP9313 Project 2**.\n",
    "\n",
    "* You are required to complete your implementation in the file `submission.py` provided along with this notebook.\n",
    "\n",
    "* You are not allowed to print out unnecessary stuff. We will not consider any output printed out on the screen. All results should be returned in appropriate data structures via corresponding functions.\n",
    "\n",
    "* You are required to submit the following files, via CSE `give`: \n",
    "    - (i)`submission.py`(your code), \n",
    "<!--     - (ii)`model.tar.gz` (your trained model) -->\n",
    "    - (ii)`report.pdf` (illustrating your implementation details)\n",
    "    - **Note:** detailed submission instructions will be announced later.\n",
    "\n",
    "* We provide you with detailed instructions for the project in this notebook. In case of any problem, you can post your query @Piazza. Please do not post questions regarding the implementation details.\n",
    "\n",
    "* You are allowed to add other functions and/or import modules (you may have to for this project), but you are not allowed to define global variables. **All the functions should be implemented in `submission.py`**. \n",
    "\n",
    "* In this project, you may need to test your model on the provided development dataset in order to evaluate the performance of your stacking model. \n",
    "\n",
    "* The testing environment is the same as that of `Lab3`. **Note:** Importing other modules (not a part of the Lab3 test environment) may lead to errors, which will result in **ZERO score for the ENTIRE Project**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1: Stacking Model (90 points)\n",
    "\n",
    "In this task, you will implement several core parts of the stacking machine learning method in Pyspark. More specifically, you are required to complete a series of functions in the file `submission.py` with methods in PySpark SQL and PySpark MLlib. Details are listed as follows:\n",
    "\n",
    "### Dataset Description\n",
    "1. The dataset consists of sentences from customer reviews of different restaurants. There are 2241, 800, 800 customer reviews in the train, development, and test datasets, respectively. It should be noted that there is at least one sentence in each customer review and each customer review may not be with ending punctuation such as `.`, `?`.\n",
    "2. The task is to identify the category of each customer review using the review text and the trained model.\n",
    "3. The categories include:\n",
    "    * FOOD: reviews that involve comments on the food. \n",
    "        - e.g. “All the appetizers and salads were fabulous , the steak was mouth watering and the pasta was delicious”\n",
    "    * PAS: reviews that only involve comments on price, ambience, or service. \n",
    "        - e.g. “Now it 's so crowded and loud you ca n't even talk to the person next to you”\n",
    "    * MISC: reviews that do not belong to the above categories including sentences that are general recommendations  reviews describing the reviewer’s personal experience or context, but that do not usually provide information on the restaurant quality \n",
    "        - e.g. “Your friends will thank you for introducing them to this gem!”\n",
    "        - e.g. “I knew upon visiting NYC that I wanted to try an original deli”\n",
    "2. You can view samples from the dataset using `dataset.show()` to get five samples with `descript` column showing the review text and `category` column showing the annotated class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 (30 points): Build a Preprocessing Pipeline\n",
    "In this task, you need to complete the `base_features_gen_pipeline()` function in `submission.py`, which outputs a pipeline (**NOTE**: not a pipeline model). The returned pipeline will be used to process the data, construct the feature vectors and labels. 处理数据，构建特征向量和标签。\n",
    "\n",
    "More specifically, the function is defined as\n",
    "```python\n",
    "def build_base_features_pipeline(input_descript_col=\"descript\", input_category_col=\"category\", output_feature_col=\"features\", output_label_col=\"label\"):\n",
    "```\n",
    "The function needs to tokenize each customer review (i.e., the `descript`) and generate bag of words count vectors as `features`. It also needs to convert the `category` into `label` which is an integer between 0 and 2.\n",
    "该功能需要标记每个客户评论（即描述），并生成袋字数向量作为特征。 它还需要将类别转换为标签，标签是介于0到2之间的整数\n",
    "\n",
    "The returned type of this function should be `pyspark.ml.pipeline.Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2 (30 points): Generate Meta Features for Training\n",
    "In this task, you need to complete the `gen_meta_features()` function in `submission.py`, which outputs a dataframe with generated meta features for training the meta classifier. 产生元数据特征用于训练元数据分类器\n",
    "\n",
    "More specifically, the function is defined as\n",
    "```python\n",
    "def gen_meta_features(training_df, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2):\n",
    "```\n",
    "\n",
    "The description of **input** parameters are as below:\n",
    "* `training_df`: the dataframe contains features, labels, and group ids for training data. The schema of `training_df` is:\n",
    "```\n",
    "root\n",
    " |-- id: integer (nullable = true)\n",
    " |-- features: vector (nullable = true)\n",
    " |-- label: double (nullable = false)\n",
    " |-- label_0: double (nullable = false)\n",
    " |-- label_1: double (nullable = false)\n",
    " |-- label_2: double (nullable = false)\n",
    " |-- group: integer (nullable = true)\n",
    "```\n",
    "where `features` and `label` are generated using the pipeline built in Task 1.1. `label_x` corresponds to the binary label of label x (e.g., `label_0==0` means that `label!=0`). `group` is the group id as defined in the lecture slides (i.e., L7P45).\n",
    "\n",
    "* nb_x: the predefined x-th Naive Bayes model (i.e., the one will be trained using `label_x`) 预定义的第x个朴素贝叶斯模型（即将使用“ label_x”进行训练）\n",
    "* svm_x: the predefined x-th SVM model (i.e., the one will be trained using `label_x`) 预定义的第x个SVM模型（即将使用`label_x`进行训练）\n",
    "\n",
    "The **output** of the function is a dataframe with the following schema:\n",
    "```\n",
    "root\n",
    " |-- id: integer (nullable = true)\n",
    " |-- group: integer (nullable = true)\n",
    " |-- features: vector (nullable = true)\n",
    " |-- label: double (nullable = false)\n",
    " |-- label_0: double (nullable = false)\n",
    " |-- label_1: double (nullable = false)\n",
    " |-- label_2: double (nullable = false)\n",
    " |-- nb_pred_0: double (nullable = false)\n",
    " |-- nb_pred_1: double (nullable = false)\n",
    " |-- nb_pred_2: double (nullable = false)\n",
    " |-- svm_pred_0: double (nullable = false)\n",
    " |-- svm_pred_1: double (nullable = false)\n",
    " |-- svm_pred_2: double (nullable = false)\n",
    " |-- joint_pred_0: double (nullable = false)\n",
    " |-- joint_pred_1: double (nullable = false)\n",
    " |-- joint_pred_2: double (nullable = false)\n",
    "```\n",
    "where `nb_pred_x` is the prediction of model `nb_x`, `svm_pred_x` is the prediction of model `svm_x`, and `joint_pred_x` is the joint prediction of model `nb_x` and `svm_x`.  在第二步又添加了9个新的列\n",
    "\n",
    "More specifically, the value of `joint_pred_x` is the decimal number of the joint prediction in L7P51 (hence it ranges from 0 to 3). E.g., if `nb_pred_1==1` and `svm_pred_1==0`, then `joint_pred_1==2`.  joint_pred_x 这一列是十进制数字，需要做一步转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3 (30 points): Obtain the prediction for the test data\n",
    "In this task, you need to complete the `test_prediction()` function in `submission.py`, which outputs a dataframe with predicted labels of the test data.\n",
    "\n",
    "More specifically, the function is defined as\n",
    "```python\n",
    "def test_prediction(test_df, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier):\n",
    "```\n",
    "\n",
    "The description of **input** parameters are as below:\n",
    "* `test_df`: the dataframe contains features, labels, and group ids for test data. The schema of `training_df` is:\n",
    "```\n",
    "root\n",
    " |-- id: integer (nullable = true)\n",
    " |-- category: string (nullable = true)\n",
    " |-- descript: string (nullable = true)\n",
    "```\n",
    "\n",
    "* `base_features_pipeline_model` is the fitted pipeline model for the pipeline built in Task 1.1.\n",
    "* `gen_base_pred_pipeline_model` is the fitted pipeline model that generates predictions of base classifiers for the test data.\n",
    "* `gen_meta_feature_pipeline_model` is the fitted pipeline model that generates meta features of the data from the single and joint predictions of base classifiers\n",
    "* `meta_classifier` is the fitted meta classifier. \n",
    "* you will see how we declare all the above 3 pipeline models in the examples below.\n",
    "\n",
    "The **output** of the function is a dataframe with the following schema:\n",
    "```\n",
    "root\n",
    " |-- id: integer (nullable = true)\n",
    " |-- label: double (nullable = false)\n",
    " |-- final_prediction: double (nullable = false)\n",
    "```\n",
    "where `labels` are generated using the pipeline built in Task 1.1, and `final_prediction` is the prediction result of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The evaluation of the project is based on the correctness of your implementation. The three subtasks will be tested independently, i.e., even if you don't complete task 1.1 and task 1.2, you may still get 30 points, if you have correctly implemented task 1.3.\n",
    "\n",
    "Similar to Project 1, we will set a very loose time threshold T just in case your code takes long to complete... **If your implementation does not finish prediction in a certain time, it will be killed. Hence, 0 score.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Report (10 points)\n",
    "You are also required to submit a report named `report.pdf`. Specifically, in the report, you are at least expected to answer the following questions:\n",
    "\n",
    "1. Evaluation of your stacking model on the test data. \n",
    "2. How would you improve the performance (e.g., F1) of the stacking model.\n",
    "\n",
    "For task 2.2, you may try from the following directions:\n",
    "* the base feature generation\n",
    "* the meta feature generation\n",
    "* the hyper-parameters of base and meta models\n",
    "\n",
    "**Hint**: make proper use of the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame操作指南\n",
    "https://blog.csdn.net/sinat_26917383/article/details/80500349  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to execute your implementation (EXAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7483312619309965\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from submission import base_features_gen_pipeline, gen_meta_features, test_prediction\n",
    "\n",
    "import random\n",
    "rseed = 1024\n",
    "random.seed(rseed)\n",
    "\n",
    "#创建新的3列\n",
    "def gen_binary_labels(df):\n",
    "    df = df.withColumn('label_0', (df['label'] == 0).cast(DoubleType()))\n",
    "    df = df.withColumn('label_1', (df['label'] == 1).cast(DoubleType()))\n",
    "    df = df.withColumn('label_2', (df['label'] == 2).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Create a Spark Session\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Load data\n",
    "train_data = spark.read.load(\"proj2train.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "test_data = spark.read.load(\"proj2test.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# build the pipeline from task 1.1 开始使用 submission 里面定义的函数\n",
    "base_features_pipeline = base_features_gen_pipeline()\n",
    "\n",
    "# Fit the pipeline using train_data， fit一下进行训练\n",
    "base_features_pipeline_model = base_features_pipeline.fit(train_data)\n",
    "#fit 完了之后才能进行转换 transfrom \n",
    "\n",
    "# Transform the train_data using fitted pipeline\n",
    "training_set = base_features_pipeline_model.transform(train_data)\n",
    "\n",
    "#开始转化lable,变为3种类别的label\n",
    "# assign random groups and binarize the labels  分为5组\n",
    "training_set = training_set.withColumn('group', (rand(rseed)*5).cast(IntegerType()))\n",
    "training_set = gen_binary_labels(training_set)\n",
    "\n",
    "####################################################################################################\n",
    "#开始第二问\n",
    "#上面结束才是 会生成一个大表，第二问的输入\n",
    "\n",
    "# define base models 定义6个模型\n",
    "nb_0 = NaiveBayes(featuresCol='features', labelCol='label_0', predictionCol='nb_pred_0', probabilityCol='nb_prob_0', rawPredictionCol='nb_raw_0')\n",
    "nb_1 = NaiveBayes(featuresCol='features', labelCol='label_1', predictionCol='nb_pred_1', probabilityCol='nb_prob_1', rawPredictionCol='nb_raw_1')\n",
    "nb_2 = NaiveBayes(featuresCol='features', labelCol='label_2', predictionCol='nb_pred_2', probabilityCol='nb_prob_2', rawPredictionCol='nb_raw_2')\n",
    "svm_0 = LinearSVC(featuresCol='features', labelCol='label_0', predictionCol='svm_pred_0', rawPredictionCol='svm_raw_0')\n",
    "svm_1 = LinearSVC(featuresCol='features', labelCol='label_1', predictionCol='svm_pred_1', rawPredictionCol='svm_raw_1')\n",
    "svm_2 = LinearSVC(featuresCol='features', labelCol='label_2', predictionCol='svm_pred_2', rawPredictionCol='svm_raw_2')\n",
    "\n",
    "#在第二问中也要建立一个 pipeline\n",
    "# build pipeline to generate predictions from base classifiers, will be used in task 1.3\n",
    "gen_base_pred_pipeline = Pipeline(stages=[nb_0, nb_1, nb_2, svm_0, svm_1, svm_2])\n",
    "# fit 进行训练成为 model\n",
    "gen_base_pred_pipeline_model = gen_base_pred_pipeline.fit(training_set)\n",
    "#然后把测试数据放入模型，做出预测 产生 nb_pred_0, svm_pred_0, joint_pred_0.  会一共生成9列，然后每个group都要进行这些步骤\n",
    "# task 1.2 \n",
    "meta_features = gen_meta_features(training_set, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2)\n",
    "\n",
    "\n",
    "#我们需要的9个column产生之后，就是第三问的输入\n",
    "#9个column要进一步的处理合并变成 meta feature\n",
    "# build onehotencoder and vectorassembler pipeline  \n",
    "onehot_encoder = OneHotEncoderEstimator(inputCols=['nb_pred_0', 'nb_pred_1', 'nb_pred_2', 'svm_pred_0', 'svm_pred_1', 'svm_pred_2', 'joint_pred_0', 'joint_pred_1', 'joint_pred_2'], outputCols=['vec{}'.format(i) for i in range(9)])\n",
    "#横向拼接\n",
    "vector_assembler = VectorAssembler(inputCols=['vec{}'.format(i) for i in range(9)], outputCol='meta_features')\n",
    "#又重新建立一个pipeline 串在一起   fit 一下， transform一下\n",
    "gen_meta_feature_pipeline = Pipeline(stages=[onehot_encoder, vector_assembler])\n",
    "gen_meta_feature_pipeline_model = gen_meta_feature_pipeline.fit(meta_features)\n",
    "\n",
    "meta_features = gen_meta_feature_pipeline_model.transform(meta_features)\n",
    "\n",
    "#得到我们第三问需要的mata_feature\n",
    "# train the meta clasifier\n",
    "lr_model = LogisticRegression(featuresCol='meta_features', labelCol='label', predictionCol='final_prediction', maxIter=20, regParam=1., elasticNetParam=0)\n",
    "meta_classifier = lr_model.fit(meta_features)\n",
    "\n",
    "# task 1.3\n",
    "pred_test = test_prediction(test_data, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",metricName='f1')\n",
    "print(evaluator.evaluate(pred_test, {evaluator.predictionCol:'final_prediction'}))\n",
    "spark.stop()\n",
    "#train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- list_descript: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+\n",
      "| id|category|            descript|       list_descript|            features|label|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+\n",
      "|  0|    MISC|I've been there t...|[i've, been, ther...|(5421,[1,18,31,39...|  1.0|\n",
      "|  1|    FOOD|Stay away from th...|[stay, away, from...|(5421,[0,1,15,20,...|  0.0|\n",
      "|  2|    FOOD|Wow over 100 beer...|[wow, over, 100, ...|(5421,[3,109,556,...|  0.0|\n",
      "|  3|    MISC|Having been a lon...|[having, been, a,...|(5421,[1,2,3,5,6,...|  1.0|\n",
      "|  4|    MISC|This is a consist...|[this, is, a, con...|(5421,[2,3,4,8,11...|  1.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from submission import base_features_gen_pipeline, gen_meta_features, test_prediction\n",
    "\n",
    "import random\n",
    "rseed = 1024\n",
    "random.seed(rseed)\n",
    "\n",
    "def gen_binary_labels(df):\n",
    "    df = df.withColumn('label_0', (df['label'] == 0).cast(DoubleType()))\n",
    "    df = df.withColumn('label_1', (df['label'] == 1).cast(DoubleType()))\n",
    "    df = df.withColumn('label_2', (df['label'] == 2).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Create a Spark Session\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"pro2\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# Load data\n",
    "train_data = spark.read.load(\"proj2train.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "test_data = spark.read.load(\"proj2test.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "# build the pipeline from task 1.1 开始使用 submission 里面定义的函数\n",
    "base_features_pipeline = base_features_gen_pipeline()\n",
    "# Fit the pipeline using train_data， fit一下进行训练\n",
    "base_features_pipeline_model = base_features_pipeline.fit(train_data)\n",
    "# #fit 完了之后才能进行转换 transfrom \n",
    "# # Transform the train_data using fitted pipeline\n",
    "training_set = base_features_pipeline_model.transform(train_data)\n",
    "training_set.printSchema()\n",
    "training_set.show(5)\n",
    "\n",
    "# 到这里我们可以看到，经过第一问Pipeline的转换\n",
    "#使用Tokenizer 把descript这一列转化成了 words\n",
    "# 使用CountVectorizer把words 这一列转化成了 features\n",
    "#  使用StringIndexer 把category这一列转化成了 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+\n",
      "| id|category|            descript|       list_descript|            features|label|group|label_0|label_1|label_2|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+\n",
      "|  0|    MISC|I've been there t...|[i've, been, ther...|(5421,[1,18,31,39...|  1.0|    4|    0.0|    1.0|    0.0|\n",
      "|  1|    FOOD|Stay away from th...|[stay, away, from...|(5421,[0,1,15,20,...|  0.0|    4|    1.0|    0.0|    0.0|\n",
      "|  2|    FOOD|Wow over 100 beer...|[wow, over, 100, ...|(5421,[3,109,556,...|  0.0|    4|    1.0|    0.0|    0.0|\n",
      "|  3|    MISC|Having been a lon...|[having, been, a,...|(5421,[1,2,3,5,6,...|  1.0|    0|    0.0|    1.0|    0.0|\n",
      "|  4|    MISC|This is a consist...|[this, is, a, con...|(5421,[2,3,4,8,11...|  1.0|    2|    0.0|    1.0|    0.0|\n",
      "|  5|    FOOD|I ate here a week...|[i, ate, here, a,...|(5421,[1,2,5,25,4...|  0.0|    0|    1.0|    0.0|    0.0|\n",
      "|  6|    MISC|First of all Dal ...|[first, of, all, ...|(5421,[7,40,142,1...|  1.0|    4|    0.0|    1.0|    0.0|\n",
      "|  7|    FOOD|Great food at REA...|[great, food, at,...|(5421,[8,13,19,25...|  0.0|    4|    1.0|    0.0|    0.0|\n",
      "|  8|    FOOD|While there are p...|[while, there, ar...|(5421,[2,3,7,8,21...|  0.0|    4|    1.0|    0.0|    0.0|\n",
      "|  9|    MISC|My first encounte...|[my, first, encou...|(5421,[2,16,22,49...|  1.0|    4|    0.0|    1.0|    0.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#把上面的label 转化成新的label  label_0，label_1，label_2\n",
    "training_set = training_set.withColumn('group', (rand(rseed)*5).cast(IntegerType()))\n",
    "training_set = gen_binary_labels(training_set)\n",
    "training_set.show(10)\n",
    "#training_set.printSchema()\n",
    "#第一问的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始测试第二问\n",
    "#开始staking 流程\n",
    "nb_0 = NaiveBayes(featuresCol='features', labelCol='label_0', predictionCol='nb_pred_0', probabilityCol='nb_prob_0', rawPredictionCol='nb_raw_0')\n",
    "nb_1 = NaiveBayes(featuresCol='features', labelCol='label_1', predictionCol='nb_pred_1', probabilityCol='nb_prob_1', rawPredictionCol='nb_raw_1')\n",
    "nb_2 = NaiveBayes(featuresCol='features', labelCol='label_2', predictionCol='nb_pred_2', probabilityCol='nb_prob_2', rawPredictionCol='nb_raw_2')\n",
    "svm_0 = LinearSVC(featuresCol='features', labelCol='label_0', predictionCol='svm_pred_0', rawPredictionCol='svm_raw_0')\n",
    "svm_1 = LinearSVC(featuresCol='features', labelCol='label_1', predictionCol='svm_pred_1', rawPredictionCol='svm_raw_1')\n",
    "svm_2 = LinearSVC(featuresCol='features', labelCol='label_2', predictionCol='svm_pred_2', rawPredictionCol='svm_raw_2')\n",
    "\n",
    "#在第二问中也要建立一个 pipeline\n",
    "# build pipeline to generate predictions from base classifiers, will be used in task 1.3\n",
    "gen_base_pred_pipeline = Pipeline(stages=[nb_0, nb_1, nb_2, svm_0, svm_1, svm_2])\n",
    "# fit 进行训练成为 model\n",
    "gen_base_pred_pipeline_model = gen_base_pred_pipeline.fit(training_set)\n",
    "\n",
    "#然后把测试数据放入模型，做出预测 产生 nb_pred_0, svm_pred_0, joint_pred_0.  会一共生成9列，然后每个group都要进行这些步骤\n",
    "# task 1.2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- list_descript: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- group: integer (nullable = true)\n",
      " |-- label_0: double (nullable = false)\n",
      " |-- label_1: double (nullable = false)\n",
      " |-- label_2: double (nullable = false)\n",
      " |-- nb_raw_0: vector (nullable = true)\n",
      " |-- nb_prob_0: vector (nullable = true)\n",
      " |-- nb_pred_0: double (nullable = false)\n",
      " |-- nb_raw_1: vector (nullable = true)\n",
      " |-- nb_prob_1: vector (nullable = true)\n",
      " |-- nb_pred_1: double (nullable = false)\n",
      " |-- nb_raw_2: vector (nullable = true)\n",
      " |-- nb_prob_2: vector (nullable = true)\n",
      " |-- nb_pred_2: double (nullable = false)\n",
      " |-- svm_raw_0: vector (nullable = true)\n",
      " |-- svm_pred_0: double (nullable = false)\n",
      " |-- svm_raw_1: vector (nullable = true)\n",
      " |-- svm_pred_1: double (nullable = false)\n",
      " |-- svm_raw_2: vector (nullable = true)\n",
      " |-- svm_pred_2: double (nullable = false)\n",
      " |-- joint_pred_0: double (nullable = true)\n",
      " |-- joint_pred_1: double (nullable = true)\n",
      " |-- joint_pred_2: double (nullable = true)\n",
      "\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+----------+--------------------+----------+--------------------+----------+------------+------------+------------+\n",
      "| id|category|            descript|       list_descript|            features|label|group|label_0|label_1|label_2|            nb_raw_0|           nb_prob_0|nb_pred_0|            nb_raw_1|           nb_prob_1|nb_pred_1|            nb_raw_2|           nb_prob_2|nb_pred_2|           svm_raw_0|svm_pred_0|           svm_raw_1|svm_pred_1|           svm_raw_2|svm_pred_2|joint_pred_0|joint_pred_1|joint_pred_2|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+----------+--------------------+----------+--------------------+----------+------------+------------+------------+\n",
      "|  3|    MISC|Having been a lon...|[having, been, a,...|(5421,[1,2,3,5,6,...|  1.0|    0|    0.0|    1.0|    0.0|[-141.01139678271...|[0.99767744325563...|      0.0|[-144.53260195372...|[0.01413980713894...|      1.0|[-141.65783637135...|[0.97896660175857...|      0.0|[0.27870730279756...|       0.0|[0.43310460396009...|       0.0|[1.47821276587013...|       0.0|         0.0|         2.0|         0.0|\n",
      "|  5|    FOOD|I ate here a week...|[i, ate, here, a,...|(5421,[1,2,5,25,4...|  0.0|    0|    1.0|    0.0|    0.0|[-107.32815079263...|[0.16107525768244...|      1.0|[-105.06432272859...|[0.99051257784448...|      0.0|[-103.14317586409...|[0.99996201035431...|      0.0|[-0.0028547402654...|       1.0|[1.31745447444938...|       0.0|[1.62917555468238...|       0.0|         3.0|         0.0|         0.0|\n",
      "| 14|    MISC|I will have to sa...|[i, will, have, t...|(5421,[3,4,5,11,1...|  1.0|    0|    0.0|    1.0|    0.0|[-96.330676984810...|[0.92761800113901...|      0.0|[-98.357097789040...|[0.06124614662798...|      1.0|[-95.330871253057...|[0.99721176668607...|      0.0|[1.64673316974246...|       0.0|[-0.2348410535786...|       1.0|[1.47967451692343...|       0.0|         0.0|         3.0|         0.0|\n",
      "| 21|    MISC|   THe perfect spot.|[the, perfect, sp...|(5421,[0,169,818]...|  1.0|    0|    0.0|    1.0|    0.0|[-20.287633599279...|[0.46928119127539...|      1.0|[-19.295225750563...|[0.94516220519267...|      0.0|[-20.054190466896...|[0.69392064392495...|      0.0|[0.23289801122187...|       0.0|[1.32117053598537...|       0.0|[-0.3930166283404...|       1.0|         2.0|         0.0|         1.0|\n",
      "| 22|    MISC|I was pleasantly ...|[i, was, pleasant...|(5421,[3,5,6,9,11...|  1.0|    0|    0.0|    1.0|    0.0|[-64.873615693950...|[0.99814078898908...|      0.0|[-69.927506924457...|[0.00370579627260...|      1.0|[-65.319045897353...|[0.99659522138908...|      0.0|[1.89708066368785...|       0.0|[-1.5613957276989...|       1.0|[1.46287997311065...|       0.0|         0.0|         3.0|         0.0|\n",
      "| 23|    FOOD|big and soft as w...|[big, and, soft, ...|(5421,[1,27,33,11...|  0.0|    0|    1.0|    0.0|    0.0|[-61.775685262322...|[0.05630821880133...|      1.0|[-58.218492966658...|[0.99434148812375...|      0.0|[-58.193698835895...|[0.99763074018438...|      0.0|[-2.6681860926996...|       1.0|[1.69861786568254...|       0.0|[1.87674450899265...|       0.0|         3.0|         0.0|         0.0|\n",
      "| 25|    MISC|In such a crappy ...|[in, such, a, cra...|(5421,[2,3,4,7,8,...|  1.0|    0|    0.0|    1.0|    0.0|[-113.12263180075...|[0.99541177958367...|      0.0|[-115.32802720533...|[0.67270284081021...|      0.0|[-114.05011927130...|[0.98896553494228...|      0.0|[2.96161610620412...|       0.0|[-0.4649398771178...|       1.0|[1.90642829753431...|       0.0|         0.0|         1.0|         0.0|\n",
      "| 35|    FOOD|  Have the iced tea.|[have, the, iced,...|(5421,[0,18,2612,...|  0.0|    0|    1.0|    0.0|    0.0|[-28.846689223363...|[0.51191920236514...|      0.0|[-28.876484793416...|[0.54576669425684...|      0.0|[-28.733759720462...|[0.65105959573484...|      0.0|[0.19160716700280...|       0.0|[0.24120103325443...|       0.0|[0.31328598784135...|       0.0|         0.0|         0.0|         0.0|\n",
      "| 46|    FOOD|The vibe is very ...|[the, vibe, is, v...|(5421,[0,1,4,6,13...|  0.0|    0|    1.0|    0.0|    0.0|[-91.911462912026...|[0.00667575537296...|      1.0|[-86.247930574961...|[0.99999055015163...|      0.0|[-89.072179224381...|[0.89823761531026...|      0.0|[-0.9228822704552...|       1.0|[3.76796631692680...|       0.0|[-0.2477516914071...|       1.0|         3.0|         0.0|         1.0|\n",
      "| 48|    FOOD|Their exotic sala...|[their, exotic, s...|(5421,[2,4,16,19,...|  0.0|    0|    1.0|    0.0|    0.0|[-188.04243763430...|[2.21335535765986...|      1.0|[-178.67523917194...|[0.99990995158925...|      0.0|[-179.96615336127...|[0.99663678966772...|      0.0|[-3.6002176568169...|       1.0|[3.22827451694480...|       0.0|[2.10367727624863...|       0.0|         3.0|         0.0|         0.0|\n",
      "| 53|    MISC|This restaurant i...|[this, restaurant...|(5421,[2,4,11,28,...|  1.0|    0|    0.0|    1.0|    0.0|[-40.564278058456...|[0.94632596069826...|      0.0|[-42.388322565360...|[0.25745943709153...|      1.0|[-40.137737355685...|[0.99750725639478...|      0.0|[1.34648106913189...|       0.0|[-1.3400840862846...|       1.0|[1.35072927719247...|       0.0|         0.0|         3.0|         0.0|\n",
      "| 55|    FOOD|This food is asia...|[this, food, is, ...|(5421,[4,11,13,31...|  0.0|    0|    1.0|    0.0|    0.0|[-36.210807710159...|[0.15072258259506...|      1.0|[-34.729057609152...|[0.82906584125786...|      0.0|[-34.128920391823...|[0.96939282493913...|      0.0|[-0.9516861379784...|       1.0|[0.76077149909767...|       0.0|[1.23531878469091...|       0.0|         3.0|         0.0|         0.0|\n",
      "| 61|    MISC|            Finally!|          [finally!]| (5421,[1978],[1.0])|  1.0|    0|    0.0|    1.0|    0.0|[-9.6693835279254...|[0.71574098660309...|      0.0|[-10.445433914883...|[0.34340596861978...|      1.0|[-9.6448459209493...|[0.75464499068733...|      0.0|[1.31186225164689...|       0.0|[-1.1999404331163...|       1.0|[1.31239545231288...|       0.0|         0.0|         3.0|         0.0|\n",
      "| 67|     PAS|First went here t...|[first, went, her...|(5421,[3,51,53,81...|  2.0|    0|    0.0|    0.0|    1.0|[-57.458308624845...|[0.96554344189051...|      0.0|[-59.945141245043...|[0.08641962224907...|      1.0|[-57.638846736250...|[0.98932221318462...|      0.0|[0.92682186579857...|       0.0|[-0.6075459116960...|       1.0|[0.85679329925436...|       0.0|         0.0|         3.0|         0.0|\n",
      "| 79|    MISC|Deff recimmend Bu...|[deff, recimmend,...|(5421,[18,23,78,1...|  1.0|    0|    0.0|    1.0|    0.0|[-121.75728585288...|[0.95984685602516...|      0.0|[-125.24437117463...|[0.01766046084641...|      1.0|[-122.87394622495...|[0.84742080511158...|      0.0|[2.53369641051666...|       0.0|[-0.3740144890153...|       1.0|[1.00017512125702...|       0.0|         0.0|         3.0|         0.0|\n",
      "| 80|    FOOD|Tasty but expensive.|[tasty, but, expe...|(5421,[14,205,691...|  0.0|    0|    1.0|    0.0|    0.0|[-24.760585815439...|[0.04134931820637...|      1.0|[-21.434021534892...|[0.98148602764509...|      0.0|[-21.662714720061...|[0.96383496486861...|      0.0|[0.59417825155237...|       0.0|[0.90208430577282...|       0.0|[0.09116702103738...|       0.0|         2.0|         0.0|         0.0|\n",
      "|100|    FOOD|The pizza is deli...|[the, pizza, is, ...|(5421,[0,3,4,7,23...|  0.0|    0|    1.0|    0.0|    0.0|[-150.49250091936...|[2.33801519362059...|      1.0|[-140.55246648523...|[0.99996607132904...|      0.0|[-141.67894323074...|[0.99987128069231...|      0.0|[-2.3562830161880...|       1.0|[3.45202265996975...|       0.0|[1.20672656315079...|       0.0|         3.0|         0.0|         0.0|\n",
      "|110|    FOOD|He takes real pri...|[he, takes, real,...|(5421,[1,9,13,134...|  0.0|    0|    1.0|    0.0|    0.0|[-76.086406577575...|[0.17325090366006...|      1.0|[-73.529802305614...|[0.99554916642845...|      0.0|[-75.376444424978...|[0.70020879950975...|      0.0|[-1.1904536264304...|       1.0|[1.68176896760753...|       0.0|[0.96738914781636...|       0.0|         3.0|         0.0|         0.0|\n",
      "|114|    MISC|I have been to Ca...|[i, have, been, t...|(5421,[1,2,3,5,18...|  1.0|    0|    0.0|    1.0|    0.0|[-101.43410106819...|[0.99735648889056...|      0.0|[-104.73307319334...|[0.18709414080106...|      1.0|[-101.68934596814...|[0.99939823654005...|      0.0|[1.11948988485078...|       0.0|[-0.1103225764729...|       1.0|[1.80359378783428...|       0.0|         0.0|         3.0|         0.0|\n",
      "|116|    MISC|I recommend order...|[i, recommend, or...|(5421,[2,5,9,20,7...|  1.0|    0|    0.0|    1.0|    0.0|[-43.358072530814...|[0.91295608423033...|      0.0|[-44.794068175143...|[0.27200175326716...|      1.0|[-43.545863114752...|[0.96731693005970...|      0.0|[1.02579160815673...|       0.0|[-0.3511397849005...|       1.0|[0.61531264162403...|       0.0|         0.0|         3.0|         0.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+----------+--------------------+----------+--------------------+----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features = gen_meta_features(training_set, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2)\n",
    "meta_features.printSchema()\n",
    "meta_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+----------+----------+----------+------------+------------+------------+\n",
      "|nb_pred_0|nb_pred_1|nb_pred_2|svm_pred_0|svm_pred_1|svm_pred_2|joint_pred_0|joint_pred_1|joint_pred_2|\n",
      "+---------+---------+---------+----------+----------+----------+------------+------------+------------+\n",
      "|      0.0|      1.0|      0.0|       0.0|       0.0|       0.0|         0.0|         2.0|         0.0|\n",
      "|      1.0|      0.0|      0.0|       1.0|       0.0|       0.0|         3.0|         0.0|         0.0|\n",
      "|      0.0|      1.0|      0.0|       0.0|       1.0|       0.0|         0.0|         3.0|         0.0|\n",
      "|      1.0|      0.0|      0.0|       0.0|       0.0|       1.0|         2.0|         0.0|         1.0|\n",
      "|      0.0|      1.0|      0.0|       0.0|       1.0|       0.0|         0.0|         3.0|         0.0|\n",
      "+---------+---------+---------+----------+----------+----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features.select(\"nb_pred_0\",\"nb_pred_1\",\"nb_pred_2\",\"svm_pred_0\",\"svm_pred_1\",\"svm_pred_2\",\"joint_pred_0\",\"joint_pred_1\",\"joint_pred_2\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoderEstimator(inputCols=['nb_pred_0', 'nb_pred_1', 'nb_pred_2', 'svm_pred_0', 'svm_pred_1', 'svm_pred_2', 'joint_pred_0', 'joint_pred_1', 'joint_pred_2'], outputCols=['vec{}'.format(i) for i in range(9)])\n",
    "#横向拼接\n",
    "vector_assembler = VectorAssembler(inputCols=['vec{}'.format(i) for i in range(9)], outputCol='meta_features')\n",
    "#又重新建立一个pipeline 串在一起   fit 一下， transform一下\n",
    "gen_meta_feature_pipeline = Pipeline(stages=[onehot_encoder, vector_assembler])\n",
    "gen_meta_feature_pipeline_model = gen_meta_feature_pipeline.fit(meta_features)\n",
    "\n",
    "meta_features = gen_meta_feature_pipeline_model.transform(meta_features)\n",
    "\n",
    "#得到我们第三问需要的mata_feature\n",
    "# train the meta clasifier\n",
    "lr_model = LogisticRegression(featuresCol='meta_features', labelCol='label', predictionCol='final_prediction', maxIter=20, regParam=1., elasticNetParam=0)\n",
    "meta_classifier = lr_model.fit(meta_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- list_descript: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- group: integer (nullable = true)\n",
      " |-- label_0: double (nullable = false)\n",
      " |-- label_1: double (nullable = false)\n",
      " |-- label_2: double (nullable = false)\n",
      " |-- nb_raw_0: vector (nullable = true)\n",
      " |-- nb_prob_0: vector (nullable = true)\n",
      " |-- nb_pred_0: double (nullable = false)\n",
      " |-- nb_raw_1: vector (nullable = true)\n",
      " |-- nb_prob_1: vector (nullable = true)\n",
      " |-- nb_pred_1: double (nullable = false)\n",
      " |-- nb_raw_2: vector (nullable = true)\n",
      " |-- nb_prob_2: vector (nullable = true)\n",
      " |-- nb_pred_2: double (nullable = false)\n",
      " |-- svm_raw_0: vector (nullable = true)\n",
      " |-- svm_pred_0: double (nullable = false)\n",
      " |-- svm_raw_1: vector (nullable = true)\n",
      " |-- svm_pred_1: double (nullable = false)\n",
      " |-- svm_raw_2: vector (nullable = true)\n",
      " |-- svm_pred_2: double (nullable = false)\n",
      " |-- joint_pred_0: double (nullable = true)\n",
      " |-- joint_pred_1: double (nullable = true)\n",
      " |-- joint_pred_2: double (nullable = true)\n",
      " |-- vec4: vector (nullable = true)\n",
      " |-- vec7: vector (nullable = true)\n",
      " |-- vec0: vector (nullable = true)\n",
      " |-- vec1: vector (nullable = true)\n",
      " |-- vec6: vector (nullable = true)\n",
      " |-- vec2: vector (nullable = true)\n",
      " |-- vec5: vector (nullable = true)\n",
      " |-- vec3: vector (nullable = true)\n",
      " |-- vec8: vector (nullable = true)\n",
      " |-- meta_features: vector (nullable = true)\n",
      "\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+----------+--------------------+----------+--------------------+----------+------------+------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+\n",
      "| id|category|            descript|       list_descript|            features|label|group|label_0|label_1|label_2|            nb_raw_0|           nb_prob_0|nb_pred_0|            nb_raw_1|           nb_prob_1|nb_pred_1|            nb_raw_2|           nb_prob_2|nb_pred_2|           svm_raw_0|svm_pred_0|           svm_raw_1|svm_pred_1|           svm_raw_2|svm_pred_2|joint_pred_0|joint_pred_1|joint_pred_2|         vec4|         vec7|         vec0|         vec1|         vec6|         vec2|         vec5|         vec3|         vec8|       meta_features|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+----------+--------------------+----------+--------------------+----------+------------+------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+\n",
      "|  3|    MISC|Having been a lon...|[having, been, a,...|(5421,[1,2,3,5,6,...|  1.0|    0|    0.0|    1.0|    0.0|[-141.01139678271...|[0.99767744325563...|      0.0|[-144.53260195372...|[0.01413980713894...|      1.0|[-141.65783637135...|[0.97896660175857...|      0.0|[0.27870730279756...|       0.0|[0.43310460396009...|       0.0|[1.47821276587013...|       0.0|         0.0|         2.0|         0.0|(1,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,4,5,6,...|\n",
      "|  5|    FOOD|I ate here a week...|[i, ate, here, a,...|(5421,[1,2,5,25,4...|  0.0|    0|    1.0|    0.0|    0.0|[-107.32815079263...|[0.16107525768244...|      1.0|[-105.06432272859...|[0.99051257784448...|      0.0|[-103.14317586409...|[0.99996201035431...|      0.0|[-0.0028547402654...|       1.0|[1.31745447444938...|       0.0|[1.62917555468238...|       0.0|         3.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(15,[1,2,4,5,9,12...|\n",
      "| 14|    MISC|I will have to sa...|[i, will, have, t...|(5421,[3,4,5,11,1...|  1.0|    0|    0.0|    1.0|    0.0|[-96.330676984810...|[0.92761800113901...|      0.0|[-98.357097789040...|[0.06124614662798...|      1.0|[-95.330871253057...|[0.99721176668607...|      0.0|[1.64673316974246...|       0.0|[-0.2348410535786...|       1.0|[1.47967451692343...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "| 21|    MISC|   THe perfect spot.|[the, perfect, sp...|(5421,[0,169,818]...|  1.0|    0|    0.0|    1.0|    0.0|[-20.287633599279...|[0.46928119127539...|      1.0|[-19.295225750563...|[0.94516220519267...|      0.0|[-20.054190466896...|[0.69392064392495...|      0.0|[0.23289801122187...|       0.0|[1.32117053598537...|       0.0|[-0.3930166283404...|       1.0|         2.0|         0.0|         1.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|(3,[1],[1.0])|(15,[1,2,3,4,8,9,...|\n",
      "| 22|    MISC|I was pleasantly ...|[i, was, pleasant...|(5421,[3,5,6,9,11...|  1.0|    0|    0.0|    1.0|    0.0|[-64.873615693950...|[0.99814078898908...|      0.0|[-69.927506924457...|[0.00370579627260...|      1.0|[-65.319045897353...|[0.99659522138908...|      0.0|[1.89708066368785...|       0.0|[-1.5613957276989...|       1.0|[1.46287997311065...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "| 23|    FOOD|big and soft as w...|[big, and, soft, ...|(5421,[1,27,33,11...|  0.0|    0|    1.0|    0.0|    0.0|[-61.775685262322...|[0.05630821880133...|      1.0|[-58.218492966658...|[0.99434148812375...|      0.0|[-58.193698835895...|[0.99763074018438...|      0.0|[-2.6681860926996...|       1.0|[1.69861786568254...|       0.0|[1.87674450899265...|       0.0|         3.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(15,[1,2,4,5,9,12...|\n",
      "| 25|    MISC|In such a crappy ...|[in, such, a, cra...|(5421,[2,3,4,7,8,...|  1.0|    0|    0.0|    1.0|    0.0|[-113.12263180075...|[0.99541177958367...|      0.0|[-115.32802720533...|[0.67270284081021...|      0.0|[-114.05011927130...|[0.98896553494228...|      0.0|[2.96161610620412...|       0.0|[-0.4649398771178...|       1.0|[1.90642829753431...|       0.0|         0.0|         1.0|         0.0|    (1,[],[])|(3,[1],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,1,2,3,5,6,...|\n",
      "| 35|    FOOD|  Have the iced tea.|[have, the, iced,...|(5421,[0,18,2612,...|  0.0|    0|    1.0|    0.0|    0.0|[-28.846689223363...|[0.51191920236514...|      0.0|[-28.876484793416...|[0.54576669425684...|      0.0|[-28.733759720462...|[0.65105959573484...|      0.0|[0.19160716700280...|       0.0|[0.24120103325443...|       0.0|[0.31328598784135...|       0.0|         0.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|[1.0,1.0,1.0,1.0,...|\n",
      "| 46|    FOOD|The vibe is very ...|[the, vibe, is, v...|(5421,[0,1,4,6,13...|  0.0|    0|    1.0|    0.0|    0.0|[-91.911462912026...|[0.00667575537296...|      1.0|[-86.247930574961...|[0.99999055015163...|      0.0|[-89.072179224381...|[0.89823761531026...|      0.0|[-0.9228822704552...|       1.0|[3.76796631692680...|       0.0|[-0.2477516914071...|       1.0|         3.0|         0.0|         1.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|    (1,[],[])|(3,[1],[1.0])|(15,[1,2,4,9,13],...|\n",
      "| 48|    FOOD|Their exotic sala...|[their, exotic, s...|(5421,[2,4,16,19,...|  0.0|    0|    1.0|    0.0|    0.0|[-188.04243763430...|[2.21335535765986...|      1.0|[-178.67523917194...|[0.99990995158925...|      0.0|[-179.96615336127...|[0.99663678966772...|      0.0|[-3.6002176568169...|       1.0|[3.22827451694480...|       0.0|[2.10367727624863...|       0.0|         3.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(15,[1,2,4,5,9,12...|\n",
      "| 53|    MISC|This restaurant i...|[this, restaurant...|(5421,[2,4,11,28,...|  1.0|    0|    0.0|    1.0|    0.0|[-40.564278058456...|[0.94632596069826...|      0.0|[-42.388322565360...|[0.25745943709153...|      1.0|[-40.137737355685...|[0.99750725639478...|      0.0|[1.34648106913189...|       0.0|[-1.3400840862846...|       1.0|[1.35072927719247...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "| 55|    FOOD|This food is asia...|[this, food, is, ...|(5421,[4,11,13,31...|  0.0|    0|    1.0|    0.0|    0.0|[-36.210807710159...|[0.15072258259506...|      1.0|[-34.729057609152...|[0.82906584125786...|      0.0|[-34.128920391823...|[0.96939282493913...|      0.0|[-0.9516861379784...|       1.0|[0.76077149909767...|       0.0|[1.23531878469091...|       0.0|         3.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(15,[1,2,4,5,9,12...|\n",
      "| 61|    MISC|            Finally!|          [finally!]| (5421,[1978],[1.0])|  1.0|    0|    0.0|    1.0|    0.0|[-9.6693835279254...|[0.71574098660309...|      0.0|[-10.445433914883...|[0.34340596861978...|      1.0|[-9.6448459209493...|[0.75464499068733...|      0.0|[1.31186225164689...|       0.0|[-1.1999404331163...|       1.0|[1.31239545231288...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "| 67|     PAS|First went here t...|[first, went, her...|(5421,[3,51,53,81...|  2.0|    0|    0.0|    0.0|    1.0|[-57.458308624845...|[0.96554344189051...|      0.0|[-59.945141245043...|[0.08641962224907...|      1.0|[-57.638846736250...|[0.98932221318462...|      0.0|[0.92682186579857...|       0.0|[-0.6075459116960...|       1.0|[0.85679329925436...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "| 79|    MISC|Deff recimmend Bu...|[deff, recimmend,...|(5421,[18,23,78,1...|  1.0|    0|    0.0|    1.0|    0.0|[-121.75728585288...|[0.95984685602516...|      0.0|[-125.24437117463...|[0.01766046084641...|      1.0|[-122.87394622495...|[0.84742080511158...|      0.0|[2.53369641051666...|       0.0|[-0.3740144890153...|       1.0|[1.00017512125702...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "| 80|    FOOD|Tasty but expensive.|[tasty, but, expe...|(5421,[14,205,691...|  0.0|    0|    1.0|    0.0|    0.0|[-24.760585815439...|[0.04134931820637...|      1.0|[-21.434021534892...|[0.98148602764509...|      0.0|[-21.662714720061...|[0.96383496486861...|      0.0|[0.59417825155237...|       0.0|[0.90208430577282...|       0.0|[0.09116702103738...|       0.0|         2.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|(3,[2],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[1,2,3,4,5,8,...|\n",
      "|100|    FOOD|The pizza is deli...|[the, pizza, is, ...|(5421,[0,3,4,7,23...|  0.0|    0|    1.0|    0.0|    0.0|[-150.49250091936...|[2.33801519362059...|      1.0|[-140.55246648523...|[0.99996607132904...|      0.0|[-141.67894323074...|[0.99987128069231...|      0.0|[-2.3562830161880...|       1.0|[3.45202265996975...|       0.0|[1.20672656315079...|       0.0|         3.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(15,[1,2,4,5,9,12...|\n",
      "|110|    FOOD|He takes real pri...|[he, takes, real,...|(5421,[1,9,13,134...|  0.0|    0|    1.0|    0.0|    0.0|[-76.086406577575...|[0.17325090366006...|      1.0|[-73.529802305614...|[0.99554916642845...|      0.0|[-75.376444424978...|[0.70020879950975...|      0.0|[-1.1904536264304...|       1.0|[1.68176896760753...|       0.0|[0.96738914781636...|       0.0|         3.0|         0.0|         0.0|(1,[0],[1.0])|(3,[0],[1.0])|    (1,[],[])|(1,[0],[1.0])|    (3,[],[])|(1,[0],[1.0])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(15,[1,2,4,5,9,12...|\n",
      "|114|    MISC|I have been to Ca...|[i, have, been, t...|(5421,[1,2,3,5,18...|  1.0|    0|    0.0|    1.0|    0.0|[-101.43410106819...|[0.99735648889056...|      0.0|[-104.73307319334...|[0.18709414080106...|      1.0|[-101.68934596814...|[0.99939823654005...|      0.0|[1.11948988485078...|       0.0|[-0.1103225764729...|       1.0|[1.80359378783428...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "|116|    MISC|I recommend order...|[i, recommend, or...|(5421,[2,5,9,20,7...|  1.0|    0|    0.0|    1.0|    0.0|[-43.358072530814...|[0.91295608423033...|      0.0|[-44.794068175143...|[0.27200175326716...|      1.0|[-43.545863114752...|[0.96731693005970...|      0.0|[1.02579160815673...|       0.0|[-0.3511397849005...|       1.0|[0.61531264162403...|       0.0|         0.0|         3.0|         0.0|    (1,[],[])|    (3,[],[])|(1,[0],[1.0])|    (1,[],[])|(3,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(1,[0],[1.0])|(3,[0],[1.0])|(15,[0,2,3,5,6,12...|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+-----+-------+-------+-------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+--------------------+---------+--------------------+----------+--------------------+----------+--------------------+----------+------------+------------+------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task 1.3\n",
    "pred_test = test_prediction(test_data, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier)\n",
    "pred_test.printSchema()\n",
    "pred_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7483312619309965\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",metricName='f1')\n",
    "print(evaluator.evaluate(pred_test, {evaluator.predictionCol:'final_prediction'}))\n",
    "spark.stop()\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772613051470589\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from submission import base_features_gen_pipeline, gen_meta_features, test_prediction\n",
    "\n",
    "import random\n",
    "rseed = 1024\n",
    "random.seed(rseed)\n",
    "\n",
    "#创建新的3列\n",
    "def gen_binary_labels(df):\n",
    "    df = df.withColumn('label_0', (df['label'] == 0).cast(DoubleType()))\n",
    "    df = df.withColumn('label_1', (df['label'] == 1).cast(DoubleType()))\n",
    "    df = df.withColumn('label_2', (df['label'] == 2).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Create a Spark Session\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Load data\n",
    "train_data = spark.read.load(\"proj2train.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "test_data = spark.read.load(\"proj2test.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# build the pipeline from task 1.1 开始使用 submission 里面定义的函数\n",
    "base_features_pipeline = base_features_gen_pipeline()\n",
    "\n",
    "# Fit the pipeline using train_data， fit一下进行训练\n",
    "base_features_pipeline_model = base_features_pipeline.fit(test_data)\n",
    "#fit 完了之后才能进行转换 transfrom \n",
    "\n",
    "# Transform the train_data using fitted pipeline\n",
    "training_set = base_features_pipeline_model.transform(test_data)\n",
    "\n",
    "#开始转化lable,变为3种类别的label\n",
    "# assign random groups and binarize the labels  分为5组\n",
    "training_set = training_set.withColumn('group', (rand(rseed)*5).cast(IntegerType()))\n",
    "training_set = gen_binary_labels(training_set)\n",
    "\n",
    "####################################################################################################\n",
    "#开始第二问\n",
    "#上面结束才是 会生成一个大表，第二问的输入\n",
    "\n",
    "# define base models 定义6个模型\n",
    "nb_0 = NaiveBayes(featuresCol='features', labelCol='label_0', predictionCol='nb_pred_0', probabilityCol='nb_prob_0', rawPredictionCol='nb_raw_0')\n",
    "nb_1 = NaiveBayes(featuresCol='features', labelCol='label_1', predictionCol='nb_pred_1', probabilityCol='nb_prob_1', rawPredictionCol='nb_raw_1')\n",
    "nb_2 = NaiveBayes(featuresCol='features', labelCol='label_2', predictionCol='nb_pred_2', probabilityCol='nb_prob_2', rawPredictionCol='nb_raw_2')\n",
    "svm_0 = LinearSVC(featuresCol='features', labelCol='label_0', predictionCol='svm_pred_0', rawPredictionCol='svm_raw_0')\n",
    "svm_1 = LinearSVC(featuresCol='features', labelCol='label_1', predictionCol='svm_pred_1', rawPredictionCol='svm_raw_1')\n",
    "svm_2 = LinearSVC(featuresCol='features', labelCol='label_2', predictionCol='svm_pred_2', rawPredictionCol='svm_raw_2')\n",
    "\n",
    "#在第二问中也要建立一个 pipeline\n",
    "# build pipeline to generate predictions from base classifiers, will be used in task 1.3\n",
    "gen_base_pred_pipeline = Pipeline(stages=[nb_0, nb_1, nb_2, svm_0, svm_1, svm_2])\n",
    "# fit 进行训练成为 model\n",
    "gen_base_pred_pipeline_model = gen_base_pred_pipeline.fit(training_set)\n",
    "#然后把测试数据放入模型，做出预测 产生 nb_pred_0, svm_pred_0, joint_pred_0.  会一共生成9列，然后每个group都要进行这些步骤\n",
    "# task 1.2 \n",
    "meta_features = gen_meta_features(training_set, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2)\n",
    "\n",
    "\n",
    "#我们需要的9个column产生之后，就是第三问的输入\n",
    "#9个column要进一步的处理合并变成 meta feature\n",
    "# build onehotencoder and vectorassembler pipeline  \n",
    "onehot_encoder = OneHotEncoderEstimator(inputCols=['nb_pred_0', 'nb_pred_1', 'nb_pred_2', 'svm_pred_0', 'svm_pred_1', 'svm_pred_2', 'joint_pred_0', 'joint_pred_1', 'joint_pred_2'], outputCols=['vec{}'.format(i) for i in range(9)])\n",
    "#横向拼接\n",
    "vector_assembler = VectorAssembler(inputCols=['vec{}'.format(i) for i in range(9)], outputCol='meta_features')\n",
    "#又重新建立一个pipeline 串在一起   fit 一下， transform一下\n",
    "gen_meta_feature_pipeline = Pipeline(stages=[onehot_encoder, vector_assembler])\n",
    "gen_meta_feature_pipeline_model = gen_meta_feature_pipeline.fit(meta_features)\n",
    "\n",
    "meta_features = gen_meta_feature_pipeline_model.transform(meta_features)\n",
    "\n",
    "#得到我们第三问需要的mata_feature\n",
    "# train the meta clasifier\n",
    "lr_model = LogisticRegression(featuresCol='meta_features', labelCol='label',\n",
    "                              predictionCol='final_prediction', maxIter=20, regParam=1., elasticNetParam=0)\n",
    "meta_classifier = lr_model.fit(meta_features)\n",
    "\n",
    "# task 1.3\n",
    "pred_test = test_prediction(test_data, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",metricName='f1')\n",
    "print(evaluator.evaluate(pred_test, {evaluator.predictionCol:'final_prediction'}))\n",
    "spark.stop()\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Submission and Feedback\n",
    "\n",
    "For the project submission, you are required to submit the following files:\n",
    "\n",
    "1. Your implementation in the python file `submission.py`.\n",
    "<!-- 2. Your trained model (including base models, stack model, base encoder, and stack encoder) in the tar.gz file `model.tar.gz`. -->\n",
    "2. The report `report.pdf`.\n",
    "\n",
    "Detailed instructions about using `give` to submit the project files will be announced later via Piazza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '01'\n",
    "d = int(c, 2)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
