{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Indexing - StringIndexer</center>   字符串索引器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession   # 与spark 进行连接\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#固定的写法，创建一个 spark session 代表跟spark 的连接\n",
    "spark = SparkSession.builder.appName(\"index_vectorize\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#创建一个表 spark.createDataFrame\n",
    "df = spark.createDataFrame( \n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexer_be43b1c30803"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step1. create indexer object \n",
    "#  inputCol 输入变量是 category这一列    outputCol输出变量命名为  categoryIndex\n",
    "\n",
    "# string 到index 的变换\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2. fit to get indexer model - calculate what categorical value should be mapped to what index\n",
    "# can be based on frequencey from high to low, or low to high\n",
    "# another option alphabetic order from high to low, or low to high\n",
    "# fit, in essence, is calculation. \n",
    "\n",
    "model_index = indexer.fit(df)  # fit()  背后的工作，扫描全表，然后排序就直接进行对应\n",
    "                               #这一步的作用：扫描indexer全表，然后排序 与 0 1 2 3 直接进行对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. transform origin dataframe to indexed dataframe\n",
    "df_indexed = model_index.transform(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed.show()  #  扫描全表，然后排序就直接进行对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把index 再变回去\n",
    "from pyspark.ml.feature import IndexToString\n",
    "i2s = IndexToString().setInputCol('categoryIndex')\n",
    "res = i2s.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+----------------------------------+\n",
      "| id|category|categoryIndex|IndexToString_2dd48f059282__output|\n",
      "+---+--------+-------------+----------------------------------+\n",
      "|  0|       a|          0.0|                                 a|\n",
      "|  1|       b|          2.0|                                 b|\n",
      "|  2|       c|          1.0|                                 c|\n",
      "|  3|       a|          0.0|                                 a|\n",
      "|  4|       a|          0.0|                                 a|\n",
      "|  5|       c|          1.0|                                 c|\n",
      "+---+--------+-------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Encoder - OneHotEncoder</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator  # 一个类 当你把列款占城多个列的时候只有一个地方是 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. create encoder object\n",
    "encoder = OneHotEncoderEstimator(inputCols=['categoryIndex'], outputCols=['categoryVec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2. fit to get encoder model\n",
    "model_encoder = encoder.fit(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. transform indexed dataframe to vectorized dataframe\n",
    "df_encoded = model_encoder.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encoded.show()   # 稀疏矩阵 (2,[0],[1.0]） 有两位  第0位有值 值为 1.0  [1.0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 -> [1.0, 0]\n",
    "# 1.0 -> [0, 1.0]\n",
    "# 2.0 -> [0.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Vectorization - VectorAssembler</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler  # 从spark 把这个类导入，这个类里面自动进行转化了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = VectorAssembler(inputCols=['categoryVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized = vectorizer.transform(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+---------+\n",
      "| id|category|categoryIndex|  categoryVec| features|\n",
      "+---+--------+-------------+-------------+---------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|\n",
      "|  1|       b|          2.0|    (2,[],[])|(2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|\n",
      "+---+--------+-------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectorized.show()  #  转变为一个  list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Try One More Feature</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|category2|\n",
      "+---+---------+\n",
      "|  0|        H|\n",
      "|  1|        M|\n",
      "|  2|        L|\n",
      "|  3|        H|\n",
      "|  4|        H|\n",
      "|  5|        L|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [(0, \"H\"), (1, \"M\"), (2, \"L\"), (3, \"H\"), (4, \"H\"), (5, \"L\")],\n",
    "    [\"id\", \"category2\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+---------+\n",
      "| id|category|categoryIndex|  categoryVec| features|\n",
      "+---+--------+-------------+-------------+---------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|\n",
      "|  1|       b|          2.0|    (2,[],[])|(2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|\n",
      "+---+--------+-------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectorized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_vectorized.join(df2, 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+---------+---------+\n",
      "| id|category|categoryIndex|  categoryVec| features|category2|\n",
      "+---+--------+-------------+-------------+---------+---------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|\n",
      "|  1|       b|          2.0|    (2,[],[])|(2,[],[])|        M|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|\n",
      "+---+--------+-------------+-------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "indexer2 = StringIndexer(inputCol=\"category2\", outputCol=\"categoryIndex2\")\n",
    "model_index2 = indexer2.fit(df2)\n",
    "df_indexed2 = model_index2.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+---------+---------+--------------+\n",
      "| id|category|categoryIndex|  categoryVec| features|category2|categoryIndex2|\n",
      "+---+--------+-------------+-------------+---------+---------+--------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|           1.0|\n",
      "|  1|       b|          2.0|    (2,[],[])|(2,[],[])|        M|           2.0|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|           1.0|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|\n",
      "+---+--------+-------------+-------------+---------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode 把categoryIndex2变成 稀疏矩阵\n",
    "encoder2 = OneHotEncoderEstimator(inputCols=['categoryIndex2'], outputCols=['categoryVec2'])\n",
    "\n",
    "model_encoder2 = encoder2.fit(df_indexed2)\n",
    "df_encoded2 = model_encoder2.transform(df_indexed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+---------+---------+--------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec| features|category2|categoryIndex2| categoryVec2|\n",
      "+---+--------+-------------+-------------+---------+---------+--------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|           1.0|(2,[1],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|(2,[],[])|        M|           2.0|    (2,[],[])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|(2,[0],[1.0])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|           1.0|(2,[1],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|(2,[0],[1.0])|\n",
      "+---+--------+-------------+-------------+---------+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_encoded2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize features2代表了前面所有的表示\n",
    "vectorizer2 = VectorAssembler(inputCols=['categoryVec', 'categoryVec2'], outputCol=\"features2\")\n",
    "df_vectorized2 = vectorizer2.transform(df_encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+---------+---------+--------------+-------------+-----------------+\n",
      "| id|category|categoryIndex|  categoryVec| features|category2|categoryIndex2| categoryVec2|        features2|\n",
      "+---+--------+-------------+-------------+---------+---------+--------------+-------------+-----------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|(2,[0],[1.0])|[1.0,0.0,1.0,0.0]|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|           1.0|(2,[1],[1.0])|[0.0,1.0,0.0,1.0]|\n",
      "|  1|       b|          2.0|    (2,[],[])|(2,[],[])|        M|           2.0|    (2,[],[])|        (4,[],[])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|(2,[0],[1.0])|[1.0,0.0,1.0,0.0]|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[0.0,1.0]|        L|           1.0|(2,[1],[1.0])|[0.0,1.0,0.0,1.0]|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[1.0,0.0]|        H|           0.0|(2,[0],[1.0])|[1.0,0.0,1.0,0.0]|\n",
      "+---+--------+-------------+-------------+---------+---------+--------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vectorized2.show()  # feature 这里的feature 跟之前一样放在 所有的值放在一个list 里面然后进行并行运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
