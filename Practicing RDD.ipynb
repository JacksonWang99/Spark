{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习对 Spark RDD 的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家联系的时候也可以用 pyspark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 pyspark 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导包 这三行是固定的格式写法\n",
    "from pyspark import SparkConf, SparkContext  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 spark 上下文（context）， 并配置\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"practice_RDD\")  # 当前阶段 setMaster 就填 local 本地模式， appName 随便写\n",
    "sc = SparkContext(conf = conf)  # sc 这个变量是使用 Spark 的入口， 如果使用 shell， shell 会自动帮你把这两部做了\n",
    "                                # scs 不能更改，惯用写法\n",
    "                                #每次直接抄过来就可以了，只更改.setAppName(\"practice_RDD\") 就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的创建 RDD 的几种方式\n",
    "1. 读文件\n",
    "2. 从 python 的 list 转化\n",
    "3. 直接创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，\n",
    "它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和\n",
    "可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了\n",
    "\n",
    "\n",
    "RDD是Spark数据结构最基本的抽象化概念之一。RDD是一个不可改写的数据体，immutable，你只可以从一个RDD经过变换\n",
    "生产出一个新的RDD，你不能直接修改RDD里的数据。如果你熟悉functional programming应该可以理解这个概念。\n",
    "一个RDD有3个必须的组成部分\n",
    "1，每个RDD都有很多块构成，这个抽象化概念是为了允许RDD被分布式处理。用白话解释一下就是你有个1000行数据的RDD，\n",
    "那这个RDD会由100块10行的部分组成，也可以由一块1000行的部分组成。关键是这些小块数据是RDD的组成部分。\n",
    "2，我开头说的，你只可以从一个RDD变换生产出另一个RDD，所以每个RDD都会记录他是从哪个RDD生出来的。\n",
    "RDD之间就产生了一个树状的关系。这个抽象化概念是为了数据恢复。假设你丢失了一块RDD数据，由于有了这个概念，\n",
    "你可以找到丢掉的数据他妈，重新计算下你丢掉的那块数据。\n",
    "3，第二点是说每个RDD都有个妈妈，第三点就是RDD是怎么生出来的。老王对RDD妈妈做了什么才造出了RDD儿子。\n",
    "有这个概念也是为了数据恢复。打个比方，RDD_son是老妈RDD_mom做了一个map(x => x*2)把老妈的每个成员翻一倍产生的。\n",
    "那这个.map(x => x*2)就是每个RDD的第三个组成部分。 \n",
    "所以经过这三点你可以看出RDD这个抽象化概念完全是为了网络分布式运算产生了。首先他可以分成很多块，\n",
    "找很多机器一人计算一块。其次，假设有台机器被偷了，有些数据丢了，你可以根据RDD的从属关系重新计算丢掉的数据。\n",
    "RDD本身是一个interface，你只要能做到以上三点就可以写自己的RDD拿到Spark上运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6f82df36f7db>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-6f82df36f7db>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Spark 中最基本的数据抽象是 RDD。\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Spark 中最基本的数据抽象是 RDD。\n",
    "\n",
    "RDD：弹性分布式数据集 (Resilient Distributed DataSet)。\n",
    "\n",
    "1，RDD 有三个基本特性\n",
    "这三个特性分别为：分区，不可变，并行操作。\n",
    "a， 分区\n",
    "每一个 RDD 包含的数据被存储在系统的不同节点上。逻辑上我们可以将 RDD 理解成一个大的数组，数组中的每个元素就\n",
    "代表一个分区 (Partition) 。在物理存储中，每个分区指向一个存储在内存或者硬盘中的数据块 (Block) ，\n",
    "其实这个数据块就是每个 task 计算出的数据块，它们可以分布在不同的节点上。所以，RDD 只是抽象意义的数据集合，\n",
    "分区内部并不会存储具体的数据，只会存储它在该 RDD 中的 index，通过该 RDD 的 ID 和分区的 index 可以唯一确定\n",
    "对应数据块的编号，然后通过底层存储层的接口提取到数据进行处理。在集群中，各个节点上的数据块会尽可能的存储在\n",
    "内存中，只有当内存没有空间时才会放入硬盘存储，这样可以最大化的减少硬盘 IO 的开销。\n",
    "\n",
    "b，不可变\n",
    "不可变性是指每个 RDD 都是只读的，它所包含的分区信息是不可变的。由于已有的 RDD 是不可变的，所以我们只有对\n",
    "现有的 RDD 进行转化 (Transformation) 操作，才能得到新的 RDD ，一步一步的计算出我们想要的结果。\n",
    "这样会带来这样的好处：我们在 RDD 的计算过程中，不需要立刻去存储计算出的数据本身，我们只要记录每个 RDD \n",
    "是经过哪些转化操作得来的，即：依赖关系，这样一方面可以提高计算效率，一方面是错误恢复会更加容易。如果在计算\n",
    "过程中，第 N 步输出的 RDD 的节点发生故障，数据丢失，那么可以根据依赖关系从第 N-1 步去重新计算出该 RDD，\n",
    "这也是 RDD 叫做**\"弹性\"**分布式数据集的一个原因。\n",
    "c，并行操作\n",
    "因为 RDD 的分区特性，所以其天然支持并行处理的特性。即不同节点上的数据可以分别被处理，然后生成一个新的 RDD。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一种\n",
    "# 读文件，按行读取\n",
    "# readFile = sc.textFile('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 6, 'asd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第二种\n",
    "# 从 python 的 list 转化\n",
    "# RDD：弹性分布式数据集 (Resilient Distributed DataSet)\n",
    "l = [1, 3, 4, 6, \"asd\"]  #python的list\n",
    "#然后使用parallelize() 这个方法，把python list 转化成rdd\n",
    "fromPythonList = sc.parallelize(l)\n",
    "#怎么把rdd 打印出来  collect() 必须这么做\n",
    "fromPythonList.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#然后使用parallelize() 这个方法，把python list 转化成rdd\n",
    "#在转化的同时还可以直接进行分区操作\n",
    "l = [1, 3, 4, 6, \"asd\"]  #python的list\n",
    "#分成3个区\n",
    "fromPythonList = sc.parallelize(l,3)\n",
    "#glom() 查看分区\n",
    "fromPythonList.glom().collect()\n",
    "#查看分区数量\n",
    "fromPythonList.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 6, 'asd']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从 python 的 list 转化\n",
    "l = [1, 3, 4, 6, \"asd\"]\n",
    "sc.parallelize(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取前3个\n",
    "fromPythonList.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第三种方式\n",
    "# 直接创建\n",
    "sc.range(0,10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意 collect 前后的类型\n",
    "`没 collect 的时候是 RDD， collect 之后就是 python 的 list 了`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd 是没有办法打印的  检查类型就是rdd\n",
    "type(sc.range(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加入 collect 就是 list\n",
    "type(sc.range(0,10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个常用的操作\n",
    "* map\n",
    "* flatmap\n",
    "* filter\n",
    "* reduce\n",
    "* reduceByKey\n",
    "* collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始的 RDD\n",
    "rdd = sc.range(0,10)\n",
    "rdd.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99, 100, 101, 102, 103, 104, 105, 106, 107, 108]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map function 计算 \n",
    "#map可以理解成 对每个元素以for循环的方式 进行计算\n",
    "#使用的时候传入的是  lambda 形式的函数 函数自己定义\n",
    "rddAfterMap = rdd.map(lambda x : x + 99)\n",
    "#上面的形式就是 rdd格式\n",
    "#一旦 collect 就是python 的list\n",
    "rddAfterMap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99, 100, 101, 102, 103, 104, 105, 106, 107, 108]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#当数据非常负责和大的时候，可以重新定义一个函数\n",
    "#注意函数输入的参数\n",
    "def myFunction(x):\n",
    "    return x + 99\n",
    "a = rdd.map(myFunction)\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (3, 6), (4, 99), (7, 101)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 初始的 RDD\n",
    " # 数组之间的计算\n",
    "rdd2 = sc.parallelize([(1, 5),(3, 6),(4, 99),(7, 101)])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 103, 108]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里进行了计算操作   x[0] + x[1]  x 的第0位加上第一位求和\n",
    "rdd2.map(lambda x : x[0] + x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FlatMap (体会 map flap 的区别)\n",
    "1.Flatmap 要求传入的函数的返回值一定是一个  集合\n",
    "2.他是返回的结果，打散之后，加入一个集合里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 99, 1, 99, 2, 99, 3, 99, 4, 99, 5, 99, 6, 99, 7, 99, 8, 99, 9, 99]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#试验一下  返回值是很多集合，但是打散放在了一起\n",
    "rdd.flatMap(lambda x : [x,99]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (3, 6), (4, 99), (7, 101)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化 定义的rdd2\n",
    "rdd2 = sc.parallelize([(1, 5),(3, 6),(4, 99),(7, 101)])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (3, 6), (4, 99), (7, 101)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda x : x).collect() # map不变,没有进行计算，一对一对的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 3, 6, 4, 99, 7, 101]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.flatMap(lambda x : x).collect()  # flatten  把数据打散，变成了单独的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6], [4, 7], [5, 100], [8, 102]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda x : [x[0] + 1, x[1] + 1]).collect() #map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 4, 7, 5, 100, 8, 102]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.flatMap(lambda x : [x[0] + 1, x[1] + 1]).collect()  # flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 103]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter ，map 和 flatMap的比较\n",
    "a= sc.parallelize([1,2,3,3,4])\n",
    "a.filter(lambda e : e%2 ==0).map(lambda e: e+99).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 103]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.flatMap(lambda e :[e+99] if e%2 ==0 else []).collect()\n",
    "#更上面结论一样，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter (返回一个 boolean ) 过滤器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#只留下奇数\n",
    "rdd.filter(lambda x:(x%2 != 0)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (3, 6), (7, 101)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd2  [(1, 5),(3, 6),(4, 99),(7, 101)]\n",
    "rdd2.filter(lambda x : x[0] & 1 ).collect()  # x[0] 是奇数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce (输入的参数一定是两个)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = sc.range(1, 10)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.reduce(lambda x,y : x + y)  # 求和，返回值是 int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReduceByKey\n",
    "操作的元素肯定存在（key,value），至少是个元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (1, 55), (3, 6), (3, 66), (4, 9), (4, 99), (7, 101)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " rdd4 = sc.parallelize([(1, 5),(1, 55), (3, 6), (3, 66), (4, 9), (4, 99), (7, 101)])\n",
    "# rdd4 = sc.parallelize([[1, 5],[1, 55], [3, 6], [3, 66], [4, 9], [4, 99], [7, 101]]) 也可以\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 60), (3, 72), (4, 108), (7, 101)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#处理的数据类型（key,value）\n",
    "#数据里面 key 和key 有一样的，把key一样的聚合在一起，然后对value做一些操作，操作自己定义\n",
    "# value相加\n",
    "rdd4.reduceByKey(lambda x, y : x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 88), ('b', 95), ('a', 91)], [('b', 93), ('a', 95), ('b', 98)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combineByKey的用法\n",
    "rdd2= sc.parallelize([(\"a\", 88),(\"b\",95),(\"a\",91),(\"b\",93),(\"a\",95),(\"b\",98)],2)\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('b', (286, 100))], [('a', (274, 93))]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.combineByKey(lambda v : (v,1), lambda c,v : (c[0]+ v, v+1), lambda c1, c2 : (c1[0]+c2[0],c1[1]+c2[1]))\n",
    "rdd3.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect 和 Lazy Evaluation (需要去 IDE 演示)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd3 [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "def f(x):\n",
    "    print(\"Mapping Once\")\n",
    "    return x + 1 \n",
    "rdd3AfterMap = rdd3.map(f) #此时 RDD 并没有真正被操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3AfterMap.collect() # 此处应该还会输出很多\"Mapping Once\"， 但是 jupyter 的问题没输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 还有很多 API 例如 Fold.. GroupByKey ...SortByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(1) PythonRDD[48] at collect at <ipython-input-59-e6bd2b5a4286>:1 []\\n |  ParallelCollectionRDD[30] at parallelize at PythonRDD.scala:195 []'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3AfterMap.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvertedIndex 倒排索引代码实操 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
